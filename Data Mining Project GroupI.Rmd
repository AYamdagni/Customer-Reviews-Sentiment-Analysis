---
title: "Data Mining Project GroupI"
output: html_document
date: "2023-05-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
##library install and import

#install.packages('stringr')
#install.packages('tm')
# install.packages('tidyverse')
# install.packages('quanteda')
# install.packages('tidytext')
# install.packages('stm')
# install.packages('stminsights')
# install.packages('lubridate')
# install.packages('cowplot')
# install.packages('scales')
# install.packages('ggthemes')
# install.packages('ggwordcloud')
#install.packages("syuzhet")
#install.packages("tibble")
library(stringr)
library(tm)
library('e1071');
library('SparseM');
library(SnowballC);
library(textstem)
library(sentimentr)
library(ggplot2)
library(syuzhet)
library(tibble)
library(wordcloud)
library(RColorBrewer)
library(randomForest)
library(tidyverse)
library(tidytext) 
library(quanteda)
library(stm)
#library(stminsights)
library(lubridate)
library(cowplot)
library(scales)
library(ggthemes)
library(ggplot2)
library(dplyr)
library(tidyr)
library(purrr)
library(ggwordcloud)
library(igraph)
```

```{r}

#read data cleaned data into tweets and Deltadata 
##This is done to increase reusability


Deltadata <- read.csv("Delta_Airline_Review_Dataset_clean1.csv")


#PERFORM DATA PREPROCESSING

#Step1 - remove unwanted characters
replace_unicode <- function(x) { iconv(x, "UTF-8", "ASCII", sub="")}
Deltadata$reviews <- replace_unicode(Deltadata$reviews)

#Create tweet/review corpus
Deltadata$reviews <- as.character(Deltadata$reviews)
# 
vector <- as.vector(Deltadata$reviews);    # Create vector
source <- VectorSource(vector); # Create source
corpus <- Corpus(source);       # Create corpus
#
# SUCH AS TRIM WHITESPACE, REMOVE PUNCTUATION, REMOVE STOPWORDS.
corpus <- tm_map(corpus,content_transformer(stripWhitespace));
corpus <- tm_map(corpus,content_transformer(tolower));
corpus <- tm_map(corpus, content_transformer(removeWords),stopwords("english"));
corpus <- tm_map(corpus,content_transformer(removePunctuation));
corpus <- tm_map(corpus,content_transformer(removeNumbers));
# 
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
corpus <- tm_map(corpus, content_transformer(removeNumPunct))
# 
# remove URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
corpus <- tm_map(corpus, content_transformer(removeURL))
#
#Perform stemming/ stem completion using lemmetization i.e. convert to root word
corpus <- tm_map(corpus, textstem::lemmatize_strings)

# retrieve cleaned text
Deltadata$reviews <- sapply(corpus, as.character)
```

#sentiment anlysis

```{r}
reviews = get_sentences(as.character(Deltadata$reviews)) # sentence boundary disambiguation 
sentiment = sentiment_by(reviews)

sentiment_df = data.frame(sentiment)
sentiment_df$star.rating = Deltadata$star.rating

(nrow(sentiment_df)) # There are 2689 rows, retained from original df

```
```{r}
# Plot a histogram of the sentiment scores
ggplot(data = sentiment, aes(x = ave_sentiment)) +
  geom_histogram(binwidth = 0.1, color = "black", fill = "gray") +
  ggtitle("Sentiment Scores of Delta airline Reviews") +
  xlab("Sentiment Score") +
  ylab("Frequency")
```

# sentiment analysis by further types
```{r}
#histogram by star rating

by(sentiment_df$ave_sentiment,sentiment_df$star.rating,summary)
qplot(sentiment$ave_sentiment,   geom="histogram",binwidth=0.1,main="Sentiment Histogram: All Reviews")
##


#add positive negative label to the reviews
sentiment_df <- transform(
  sentiment_df, label_col= ifelse(ave_sentiment < 0, "Negative", "Positive"))
sentiment_df
```
```{r}

#Sentiment Analysis - contd.


#Average sentiment of customers by seat type
sentiment_df$Seat.Type = Deltadata$Seat.Type
sentiment_df_Seat.Type1 = subset(sentiment_df,Seat.Type=="Economy Class")
sentiment_df_Seat.Type2 = subset(sentiment_df,Seat.Type=="First Class")
sentiment_df_Seat.Type3 = subset(sentiment_df,Seat.Type=="Premium Economy")
sentiment_df_Seat.Type4 = subset(sentiment_df,Seat.Type=="Business Class")
sentiment_df_Seat.Type1
(nrow(sentiment_df_Seat.Type1))
sentiment_df_Seat.Type2
(nrow(sentiment_df_Seat.Type2))
sentiment_df_Seat.Type3
(nrow(sentiment_df_Seat.Type3))
sentiment_df_Seat.Type4
(nrow(sentiment_df_Seat.Type4))

(avg_sentiment_score_Seat.Type1 = mean(sentiment_df_Seat.Type1$ave_sentiment))
(avg_sentiment_score_Seat.Type2 = mean(sentiment_df_Seat.Type2$ave_sentiment))
(avg_sentiment_score_Seat.Type3 = mean(sentiment_df_Seat.Type3$ave_sentiment))
(avg_sentiment_score_Seat.Type4 = mean(sentiment_df_Seat.Type4$ave_sentiment))


#Average sentiment of customers per traveler_types

sentiment_df$traveler_types = Deltadata$traveler_types
sentiment_df_traveler_types1 = subset(sentiment_df,traveler_types=="Solo Leisure")
sentiment_df_traveler_types2 = subset(sentiment_df,traveler_types=="Business")
sentiment_df_traveler_types3 = subset(sentiment_df,traveler_types=="Family Leisure")
sentiment_df_traveler_types4 = subset(sentiment_df,traveler_types=="Couple Leisure")
sentiment_df_traveler_types5 = subset(sentiment_df,traveler_types=="Not Specified")
sentiment_df_traveler_types1
(nrow(sentiment_df_traveler_types1))
sentiment_df_traveler_types2
(nrow(sentiment_df_traveler_types2))
sentiment_df_traveler_types3
(nrow(sentiment_df_traveler_types3))
sentiment_df_traveler_types4
(nrow(sentiment_df_traveler_types4))
sentiment_df_traveler_types5
(nrow(sentiment_df_traveler_types5))

(avg_sentiment_score_traveler_types1 = mean(sentiment_df_traveler_types1$ave_sentiment))
(avg_sentiment_score_traveler_types2 = mean(sentiment_df_traveler_types2$ave_sentiment))
(avg_sentiment_score_traveler_types3 = mean(sentiment_df_traveler_types3$ave_sentiment))
(avg_sentiment_score_traveler_types4 = mean(sentiment_df_traveler_types4$ave_sentiment))
(avg_sentiment_score_traveler_types5 = mean(sentiment_df_traveler_types5$ave_sentiment))



sentiment_df$Month = Deltadata$Month

```



```{r}

#sentiment Anlysis - contd.

#Average sentiment of customers per Quarter

sentiment_df$Month = Deltadata$Month

filter <- sentiment_df[sentiment_df$Month %in% c('January', 'February', 'March'),]
mean_month <- mean(filter$ave_sentiment)
print(paste('Q1', mean_month))

filter <- sentiment_df[sentiment_df$Month %in% c('April', 'May', 'June'),]
mean_month <- mean(filter$ave_sentiment)
print(paste('Q2', mean_month))

filter <- sentiment_df[sentiment_df$Month %in% c('July', 'August', 'September'),]
mean_month <- mean(filter$ave_sentiment)
print(paste('Q3', mean_month))

filter <- sentiment_df[sentiment_df$Month %in% c('October', 'November', 'December'),]
mean_month <- mean(filter$ave_sentiment)
print(paste('Q4', mean_month))

```


```{r}
# 
# Create TermDocumentMatrix
tdm1 <- TermDocumentMatrix(corpus)
tdm1 = removeSparseTerms(tdm1, 0.95)
#
## CREATE DOCUMENT TERM MATRIX
dtm_matrix <- t(tdm1);
dtm_df <- data.frame(sentiment_df$label_col,as.matrix(dtm_matrix))
#
#split the data into test and train
set.seed(1234)
inTrain <- sample(nrow(dtm_df),0.7*nrow(dtm_df))
traindata <- dtm_df[inTrain,]
testdata <- dtm_df[-inTrain,]

```



```{r}
#find most frequently occuring terms
(freq.terms <- findFreqTerms(tdm1, lowfreq = 1000))
```


```{r}
# Sentiment Analysis - Vizualization 

term.freq <- rowSums(as.matrix(tdm1))
term.freq <- subset(term.freq, term.freq >= 15)
df2 <- data.frame(term = names(term.freq), freq = term.freq)
df2 <- df2[order(-df2$freq), ]

# Select the top 50 words
top_50_words <- head(df2, 50)
library(ggplot2)
# Plot the bar chart for top 50 words
ggplot(top_50_words, aes(x = term, y = freq)) +
  geom_bar(stat = "identity") +
  ggtitle("Term Frequency of Delta airline Reviews") +
  xlab("Terms") +
  ylab("Count") +
  coord_flip()

```

```{r,warning=FALSE}
# Sentiment - Vizualization 

m <- as.matrix(tdm1)
pal <- brewer.pal(9, "Reds")
# calculate the frequency of words and sort it by frequency
# Remove "delta" and "flight" from word frequencies
term.freq <- term.freq[!(names(term.freq) %in% c("delta", "flight"))]

# Plot word cloud without "delta" and "flight"
wordcloud(words = names(term.freq), freq = term.freq, min.freq = 1,
          random.order = FALSE, colors = pal)
```


```{r}

## hclust ####
# Sort the term frequencies in descending order
top_terms <- head(df2, 50)

# Create a subset of the term document matrix with only the top terms
subset_tdm <- tdm1[top_terms$term, ]

# Convert the subset matrix to a distance matrix
distMatrix <- dist(scale(as.matrix(subset_tdm)))
fit <- hclust(distMatrix, method="ward.D2")

# plot dendrogram ####
plot(fit, cex=0.9, hang=-1,
     main="Word Cluster Dendrogram")
# cut tree
rect.hclust(fit, k=5)
(groups <- cutree(fit, k=5))

```




```{r}
# Create adjacency matrix for term co-occurrence
adj_matrix <- as.matrix(tdm1) %*% t(as.matrix(tdm1))

# Get the indices of the top 50 words based on frequency
top_20_indices <- order(term.freq, decreasing = TRUE)[1:20]

# Subset the adjacency matrix and term frequencies for the top 50 words
adj_matrix_top_20 <- adj_matrix[top_20_indices, top_20_indices]
term_freq_top_20 <- term.freq[top_20_indices]

# Convert adjacency matrix to igraph object
g <- graph_from_adjacency_matrix(adj_matrix_top_20, mode = "undirected", weighted = TRUE)

# Customize plot settings
plot_layout <- layout_with_fr(g)
node_colors <- brewer.pal(8, "Set2")[groups[top_20_indices]]
node_sizes <- sqrt(term_freq_top_20)

# Plot the word network
plot(g, layout = plot_layout, vertex.label = V(g)$name, vertex.size = node_sizes,
     vertex.color = node_colors, vertex.label.cex = 0.8, edge.width = 0.5,
     main = "Word Network of Top 20 Co-Occurring Terms")
```


```{r}

(findAssocs(dtm_matrix, terms = "delta", corlimit = 0.3))


(findAssocs(dtm_matrix, terms = "flight", corlimit = 0.3))
```

```{r}

#emotion Analysis

sent2 <- get_nrc_sentiment(as.character(Deltadata$reviews))

# Let's look at the corpus as a whole again:
sent3 <- as.data.frame(colSums(sent2))
sent3 <- rownames_to_column(sent3) 
colnames(sent3) <- c("emotion", "count")
ggplot(sent3, aes(x = emotion, y = count, fill = emotion)) + geom_bar(stat = "identity") + theme_minimal() + theme(legend.position="none", panel.grid.major = element_blank()) + labs( x = "Emotion", y = "Total Count") + ggtitle("Sentiment of Filght Reviews") + theme(plot.title = element_text(hjust=0.5))
```



```{r}
# TRAIN NAIVE BAYES MODEL
model <- naiveBayes(traindata[,-1],traindata[,1]);

# PREDICTION
Predictions <- predict(model,testdata[,-1])
(confusion = table(testdata[,1],Predictions))


(Acc_test = (confusion[1,1]+confusion[2,2])/sum(confusion))

```


```{r}
# TRAIN LOGISTIC REGRESSION MODEL


model2 <- glm(factor(sentiment_df.label_col)~., data=traindata, family="binomial")
summary(model2)
#
Actual <- testdata[,1]
predicted.probability <- predict(model2, type = "response", newdata=testdata[,-1]) 
## Note the predictions are probabilities
cutoff <- 0.5
Predicted <- ifelse(predicted.probability > cutoff, "Positive","Negative")
# 
(confusion <- table(Actual,Predicted))
(Acc_test = (confusion[1,1]+confusion[2,2])/sum(confusion))
## +---------------------------------------------------------------------------------------------+
```

```{r}
# RANDOM FOREST CLASSIFICATION

# adding the dependant variable 'travaeler_types' from Deltadata into the dtm matrix
dtm_df$label_col <- NULL
dtm_df$traveler <- Deltadata$traveler_types
dtm_df$traveler <- as.factor(dtm_df$traveler)

# Splitting into train and test data
set.seed(12345)
inTraining <- sample(nrow(dtm_df),0.7*nrow(dtm_df))
trainingdata <- dtm_df[inTraining,]
testdata <- dtm_df[-inTraining,]

# Training the model

rf1 = randomForest(traveler ~., data = trainingdata)

# Predicting on the test dataset:
predict_rf1 = predict(rf1, newdata = testdata)
table(testdata$traveler, predict_rf1)

# Confusion matrix giving the class error rates for each 'traveler_type'
rf1$confusion


# Length of vectors (to equalize their lengths for the purpose of creating a dataframe 'data_word')
length(trainingdata)

# varUsed gives the list of variables used for the prediction of the model (the frequency of each word used)
v <- varUsed(rf1)
v
length(v) <- length(trainingdata)
length(v)



data_word <- data.frame(words = names(trainingdata[,-234]),frequency = varUsed(rf1))
data_word

# Creating a word cloud based on the top words used for the prediction of traveler types:
ggplot(data_word, aes(label = words,
                      size = frequency,
                      color = frequency)) +
  geom_text_wordcloud_area() +
  theme_minimal() +
  scale_color_gradient(low = "darkred", high = "red")
```

Topic Modeling

```{r}
#TOPIC MODELLING

# creating a corpus from the raw data (not vector)
top <- corpus(Deltadata$reviews)

# converting the corpus into a document feature matrix; preprocessing the data
top <- dfm(top,
               stem = TRUE,
               tolower = TRUE,
               remove_punct = TRUE,
               remove_numbers =TRUE,
               verbose = TRUE,
               remove = stopwords('english'))
top

# due to large number of features; removing sparse terms outside the range of 1% to 50%
top <- dfm_trim(top, max_docfreq = 0.8,min_docfreq = 0.01,
                    docfreq_type = 'prop')
dim(top)

# converting the dfm to a structural topic model format
top_out <- convert(top, to = "stm", docvars=Deltadata)
str(top_out, max.level = 1)

# finding the top 5 topics with the words describing every topic
# K can be changed (manually or be found optimally)
stm_5 <- stm(
  documents=top_out$documents,
  vocab=top_out$vocab,
  data = top_out$meta,
  init.type = 'Spectral', #default
  K = 5,
  verbose = FALSE
)

# Summary of the Top 5 topics and the corresponding words present in them (as ascertained from the dfm)

summary(stm_5)

# Plot of the topics
plot(stm_5)


# Visualization and Model evaluation

# checking the probability of a single word appearing across topics
top_word_prob <- tidy(stm_5)

# gives the following columns: topic, term, beta
top_word_prob[,1:3]

top_word_prob %>%
  group_by(topic) %>%
  top_n(4, beta) %>%      # gives the top 4 words for each topic (in terms of their prob)
  ungroup() %>%
  mutate(topic = paste0("Topic ", topic),
         term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL, y = expression(beta),
       title = "Highest word probabilities for each topic",
       subtitle = "Different words are associated with different topics")


```


N- Grams

```{r}
# Bigram
review_bigram <- tokens(Deltadata$reviews) %>%
  tokens_remove("\\p{P}", valuetype = "regex", padding = TRUE) %>%
  tokens_remove(stopwords("english"), padding  = TRUE) %>%
  tokens_ngrams(n = 2) %>%
  dfm()
topfeatures(review_bigram)

```
```{r}

#Trigram
review_trigram <- tokens(Deltadata$reviews) %>%
  tokens_remove("\\p{P}", valuetype = "regex", padding = TRUE) %>%
  tokens_remove(stopwords("english"), padding  = TRUE) %>%
  tokens_ngrams(n = 3) %>%
  dfm()
topfeatures(review_trigram)
```

